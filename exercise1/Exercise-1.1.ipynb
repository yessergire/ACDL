{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=784 #Mnist: 28x28 images -> input dimension = 784\n",
    "hidden_dim=128 #width of the hidden layer\n",
    "output_dim=10 #10 classes (the digits in mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slight modification to ```ff_main.py``` file: the ```test``` method returns the test rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "test_batch_size= 1000\n",
    "epochs = 14\n",
    "lr = 0.1\n",
    "gamma = 0.7\n",
    "seed = 1\n",
    "log_interval = 60\n",
    "save_model=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "device = \"cpu\" # \"cuda\"\n",
    "\n",
    "train_kwargs = {'batch_size': batch_size}\n",
    "test_kwargs = {'batch_size': test_batch_size}\n",
    "if device == \"cuda\":\n",
    "    cuda_kwargs = {'num_workers': 1,\n",
    "                   'pin_memory': True,\n",
    "                   'shuffle': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=False,\n",
    "                   transform=transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False, download=False,\n",
    "                   transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "\n",
    "optimizers = {}\n",
    "optimizers['ASGD'] = optim.ASGD(model.parameters(), lr=lr)\n",
    "optimizers['SGD'] = optim.SGD(model.parameters(), lr=lr)\n",
    "optimizers['Adagrad'] = optim.Adagrad(model.parameters(), lr=lr)\n",
    "optimizers['Adam'] = optim.Adam(model.parameters(), lr=lr)\n",
    "optimizers['Adamax'] = optim.Adamax(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training using the optimezer 'ASGD'\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.010733\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 0.042886\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 0.055763\n",
      "Train Epoch: 1 [18000/60000 (30%)]\tLoss: 0.065487\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.047050\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.086525\n",
      "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 0.084188\n",
      "Train Epoch: 1 [42000/60000 (70%)]\tLoss: 0.168876\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.100660\n",
      "Train Epoch: 1 [54000/60000 (90%)]\tLoss: 0.080115\n",
      "\n",
      "Test set: Average loss: 0.1378, Accuracy: 9606/10000 (96%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.010532\n",
      "Train Epoch: 2 [6000/60000 (10%)]\tLoss: 0.041278\n",
      "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 0.054538\n",
      "Train Epoch: 2 [18000/60000 (30%)]\tLoss: 0.065596\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.047497\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.086856\n",
      "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 0.083515\n",
      "Train Epoch: 2 [42000/60000 (70%)]\tLoss: 0.168553\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.099979\n",
      "Train Epoch: 2 [54000/60000 (90%)]\tLoss: 0.079273\n",
      "\n",
      "Test set: Average loss: 0.1376, Accuracy: 9606/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.010514\n",
      "Train Epoch: 3 [6000/60000 (10%)]\tLoss: 0.040809\n",
      "Train Epoch: 3 [12000/60000 (20%)]\tLoss: 0.054168\n",
      "Train Epoch: 3 [18000/60000 (30%)]\tLoss: 0.065706\n",
      "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 0.047540\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 0.086810\n",
      "Train Epoch: 3 [36000/60000 (60%)]\tLoss: 0.083179\n",
      "Train Epoch: 3 [42000/60000 (70%)]\tLoss: 0.168327\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.099733\n",
      "Train Epoch: 3 [54000/60000 (90%)]\tLoss: 0.078960\n",
      "\n",
      "Test set: Average loss: 0.1375, Accuracy: 9609/10000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.010514\n",
      "Train Epoch: 4 [6000/60000 (10%)]\tLoss: 0.040603\n",
      "Train Epoch: 4 [12000/60000 (20%)]\tLoss: 0.053996\n",
      "Train Epoch: 4 [18000/60000 (30%)]\tLoss: 0.065771\n",
      "Train Epoch: 4 [24000/60000 (40%)]\tLoss: 0.047514\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 0.086727\n",
      "Train Epoch: 4 [36000/60000 (60%)]\tLoss: 0.082969\n",
      "Train Epoch: 4 [42000/60000 (70%)]\tLoss: 0.168141\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.099598\n",
      "Train Epoch: 4 [54000/60000 (90%)]\tLoss: 0.078806\n",
      "\n",
      "Test set: Average loss: 0.1374, Accuracy: 9610/10000 (96%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.010514\n",
      "Train Epoch: 5 [6000/60000 (10%)]\tLoss: 0.040489\n",
      "Train Epoch: 5 [12000/60000 (20%)]\tLoss: 0.053896\n",
      "Train Epoch: 5 [18000/60000 (30%)]\tLoss: 0.065810\n",
      "Train Epoch: 5 [24000/60000 (40%)]\tLoss: 0.047482\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 0.086660\n",
      "Train Epoch: 5 [36000/60000 (60%)]\tLoss: 0.082830\n",
      "Train Epoch: 5 [42000/60000 (70%)]\tLoss: 0.167996\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.099510\n",
      "Train Epoch: 5 [54000/60000 (90%)]\tLoss: 0.078721\n",
      "\n",
      "Test set: Average loss: 0.1374, Accuracy: 9613/10000 (96%)\n",
      "\n",
      "\n",
      "Training using the optimezer 'SGD'\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.010515\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 0.040371\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 0.053423\n",
      "Train Epoch: 1 [18000/60000 (30%)]\tLoss: 0.066037\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.047818\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.086419\n",
      "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 0.082883\n",
      "Train Epoch: 1 [42000/60000 (70%)]\tLoss: 0.170077\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.100125\n",
      "Train Epoch: 1 [54000/60000 (90%)]\tLoss: 0.078404\n",
      "\n",
      "Test set: Average loss: 0.1373, Accuracy: 9612/10000 (96%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.010409\n",
      "Train Epoch: 2 [6000/60000 (10%)]\tLoss: 0.040229\n",
      "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 0.053377\n",
      "Train Epoch: 2 [18000/60000 (30%)]\tLoss: 0.066017\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.047615\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.086421\n",
      "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 0.082530\n",
      "Train Epoch: 2 [42000/60000 (70%)]\tLoss: 0.169351\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.099928\n",
      "Train Epoch: 2 [54000/60000 (90%)]\tLoss: 0.078342\n",
      "\n",
      "Test set: Average loss: 0.1372, Accuracy: 9611/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.010404\n",
      "Train Epoch: 3 [6000/60000 (10%)]\tLoss: 0.040119\n",
      "Train Epoch: 3 [12000/60000 (20%)]\tLoss: 0.053278\n",
      "Train Epoch: 3 [18000/60000 (30%)]\tLoss: 0.066010\n",
      "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 0.047516\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 0.086315\n",
      "Train Epoch: 3 [36000/60000 (60%)]\tLoss: 0.082304\n",
      "Train Epoch: 3 [42000/60000 (70%)]\tLoss: 0.168858\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.099733\n",
      "Train Epoch: 3 [54000/60000 (90%)]\tLoss: 0.078315\n",
      "\n",
      "Test set: Average loss: 0.1372, Accuracy: 9612/10000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.010411\n",
      "Train Epoch: 4 [6000/60000 (10%)]\tLoss: 0.040041\n",
      "Train Epoch: 4 [12000/60000 (20%)]\tLoss: 0.053205\n",
      "Train Epoch: 4 [18000/60000 (30%)]\tLoss: 0.066007\n",
      "Train Epoch: 4 [24000/60000 (40%)]\tLoss: 0.047460\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 0.086222\n",
      "Train Epoch: 4 [36000/60000 (60%)]\tLoss: 0.082151\n",
      "Train Epoch: 4 [42000/60000 (70%)]\tLoss: 0.168501\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.099576\n",
      "Train Epoch: 4 [54000/60000 (90%)]\tLoss: 0.078297\n",
      "\n",
      "Test set: Average loss: 0.1371, Accuracy: 9612/10000 (96%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.010419\n",
      "Train Epoch: 5 [6000/60000 (10%)]\tLoss: 0.039988\n",
      "Train Epoch: 5 [12000/60000 (20%)]\tLoss: 0.053153\n",
      "Train Epoch: 5 [18000/60000 (30%)]\tLoss: 0.066003\n",
      "Train Epoch: 5 [24000/60000 (40%)]\tLoss: 0.047425\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 0.086153\n",
      "Train Epoch: 5 [36000/60000 (60%)]\tLoss: 0.082044\n",
      "Train Epoch: 5 [42000/60000 (70%)]\tLoss: 0.168248\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.099459\n",
      "Train Epoch: 5 [54000/60000 (90%)]\tLoss: 0.078285\n",
      "\n",
      "Test set: Average loss: 0.1371, Accuracy: 9613/10000 (96%)\n",
      "\n",
      "\n",
      "Training using the optimezer 'Adagrad'\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.010425\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 0.040733\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 0.053031\n",
      "Train Epoch: 1 [18000/60000 (30%)]\tLoss: 0.069636\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.047261\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.085525\n",
      "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 0.084921\n",
      "Train Epoch: 1 [42000/60000 (70%)]\tLoss: 0.179672\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.101943\n",
      "Train Epoch: 1 [54000/60000 (90%)]\tLoss: 0.078460\n",
      "\n",
      "Test set: Average loss: 0.1364, Accuracy: 9603/10000 (96%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.010454\n",
      "Train Epoch: 2 [6000/60000 (10%)]\tLoss: 0.041450\n",
      "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 0.050086\n",
      "Train Epoch: 2 [18000/60000 (30%)]\tLoss: 0.068176\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.048569\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.084726\n",
      "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 0.081815\n",
      "Train Epoch: 2 [42000/60000 (70%)]\tLoss: 0.170751\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.097492\n",
      "Train Epoch: 2 [54000/60000 (90%)]\tLoss: 0.077734\n",
      "\n",
      "Test set: Average loss: 0.1360, Accuracy: 9606/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.010495\n",
      "Train Epoch: 3 [6000/60000 (10%)]\tLoss: 0.040805\n",
      "Train Epoch: 3 [12000/60000 (20%)]\tLoss: 0.049451\n",
      "Train Epoch: 3 [18000/60000 (30%)]\tLoss: 0.066596\n",
      "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 0.048203\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 0.083343\n",
      "Train Epoch: 3 [36000/60000 (60%)]\tLoss: 0.079988\n",
      "Train Epoch: 3 [42000/60000 (70%)]\tLoss: 0.166085\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.094476\n",
      "Train Epoch: 3 [54000/60000 (90%)]\tLoss: 0.077207\n",
      "\n",
      "Test set: Average loss: 0.1358, Accuracy: 9606/10000 (96%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.010441\n",
      "Train Epoch: 4 [6000/60000 (10%)]\tLoss: 0.040442\n",
      "Train Epoch: 4 [12000/60000 (20%)]\tLoss: 0.049236\n",
      "Train Epoch: 4 [18000/60000 (30%)]\tLoss: 0.065556\n",
      "Train Epoch: 4 [24000/60000 (40%)]\tLoss: 0.047781\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 0.082380\n",
      "Train Epoch: 4 [36000/60000 (60%)]\tLoss: 0.078648\n",
      "Train Epoch: 4 [42000/60000 (70%)]\tLoss: 0.163227\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.092920\n",
      "Train Epoch: 4 [54000/60000 (90%)]\tLoss: 0.076931\n",
      "\n",
      "Test set: Average loss: 0.1356, Accuracy: 9609/10000 (96%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.010379\n",
      "Train Epoch: 5 [6000/60000 (10%)]\tLoss: 0.039983\n",
      "Train Epoch: 5 [12000/60000 (20%)]\tLoss: 0.049196\n",
      "Train Epoch: 5 [18000/60000 (30%)]\tLoss: 0.065059\n",
      "Train Epoch: 5 [24000/60000 (40%)]\tLoss: 0.047433\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 0.081780\n",
      "Train Epoch: 5 [36000/60000 (60%)]\tLoss: 0.077631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [42000/60000 (70%)]\tLoss: 0.161206\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.091544\n",
      "Train Epoch: 5 [54000/60000 (90%)]\tLoss: 0.076734\n",
      "\n",
      "Test set: Average loss: 0.1355, Accuracy: 9608/10000 (96%)\n",
      "\n",
      "\n",
      "Training using the optimezer 'Adam'\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.010328\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 1.187323\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 1.489798\n",
      "Train Epoch: 1 [18000/60000 (30%)]\tLoss: 1.441586\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.794939\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 1.286880\n",
      "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 1.098281\n",
      "Train Epoch: 1 [42000/60000 (70%)]\tLoss: 1.056878\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 1.265502\n",
      "Train Epoch: 1 [54000/60000 (90%)]\tLoss: 1.374557\n",
      "\n",
      "Test set: Average loss: 1.1910, Accuracy: 6193/10000 (62%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.110807\n",
      "Train Epoch: 2 [6000/60000 (10%)]\tLoss: 1.048395\n",
      "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 1.192937\n",
      "Train Epoch: 2 [18000/60000 (30%)]\tLoss: 0.862414\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 1.053361\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 1.308376\n",
      "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 0.989425\n",
      "Train Epoch: 2 [42000/60000 (70%)]\tLoss: 0.858546\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 1.274550\n",
      "Train Epoch: 2 [54000/60000 (90%)]\tLoss: 1.803742\n",
      "\n",
      "Test set: Average loss: 1.4603, Accuracy: 5023/10000 (50%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.343568\n",
      "Train Epoch: 3 [6000/60000 (10%)]\tLoss: 1.110777\n",
      "Train Epoch: 3 [12000/60000 (20%)]\tLoss: 1.516440\n",
      "Train Epoch: 3 [18000/60000 (30%)]\tLoss: 1.261433\n",
      "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 1.074076\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 1.369348\n",
      "Train Epoch: 3 [36000/60000 (60%)]\tLoss: 1.623876\n",
      "Train Epoch: 3 [42000/60000 (70%)]\tLoss: 1.077897\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 1.157953\n",
      "Train Epoch: 3 [54000/60000 (90%)]\tLoss: 1.516859\n",
      "\n",
      "Test set: Average loss: 1.2554, Accuracy: 5868/10000 (59%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.055341\n",
      "Train Epoch: 4 [6000/60000 (10%)]\tLoss: 0.964942\n",
      "Train Epoch: 4 [12000/60000 (20%)]\tLoss: 1.125267\n",
      "Train Epoch: 4 [18000/60000 (30%)]\tLoss: 1.022192\n",
      "Train Epoch: 4 [24000/60000 (40%)]\tLoss: 0.819331\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 0.883464\n",
      "Train Epoch: 4 [36000/60000 (60%)]\tLoss: 1.068217\n",
      "Train Epoch: 4 [42000/60000 (70%)]\tLoss: 0.773298\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 1.023118\n",
      "Train Epoch: 4 [54000/60000 (90%)]\tLoss: 1.373779\n",
      "\n",
      "Test set: Average loss: 1.1377, Accuracy: 6051/10000 (61%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.003979\n",
      "Train Epoch: 5 [6000/60000 (10%)]\tLoss: 0.884310\n",
      "Train Epoch: 5 [12000/60000 (20%)]\tLoss: 1.155715\n",
      "Train Epoch: 5 [18000/60000 (30%)]\tLoss: 0.940805\n",
      "Train Epoch: 5 [24000/60000 (40%)]\tLoss: 0.952948\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 0.969495\n",
      "Train Epoch: 5 [36000/60000 (60%)]\tLoss: 1.007318\n",
      "Train Epoch: 5 [42000/60000 (70%)]\tLoss: 0.888609\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.942920\n",
      "Train Epoch: 5 [54000/60000 (90%)]\tLoss: 1.045657\n",
      "\n",
      "Test set: Average loss: 0.8888, Accuracy: 7606/10000 (76%)\n",
      "\n",
      "\n",
      "Training using the optimezer 'Adamax'\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.668537\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 0.710154\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 0.886918\n",
      "Train Epoch: 1 [18000/60000 (30%)]\tLoss: 0.703385\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.771428\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 1.045644\n",
      "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 0.842032\n",
      "Train Epoch: 1 [42000/60000 (70%)]\tLoss: 0.741264\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.669280\n",
      "Train Epoch: 1 [54000/60000 (90%)]\tLoss: 0.859679\n",
      "\n",
      "Test set: Average loss: 0.8432, Accuracy: 7780/10000 (78%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.489530\n",
      "Train Epoch: 2 [6000/60000 (10%)]\tLoss: 0.417574\n",
      "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 0.711944\n",
      "Train Epoch: 2 [18000/60000 (30%)]\tLoss: 0.693475\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.503075\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.893866\n",
      "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 0.833203\n",
      "Train Epoch: 2 [42000/60000 (70%)]\tLoss: 0.657566\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.546373\n",
      "Train Epoch: 2 [54000/60000 (90%)]\tLoss: 0.786934\n",
      "\n",
      "Test set: Average loss: 0.7239, Accuracy: 7919/10000 (79%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.406921\n",
      "Train Epoch: 3 [6000/60000 (10%)]\tLoss: 0.404335\n",
      "Train Epoch: 3 [12000/60000 (20%)]\tLoss: 0.709506\n",
      "Train Epoch: 3 [18000/60000 (30%)]\tLoss: 0.680984\n",
      "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 0.416065\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 0.723167\n",
      "Train Epoch: 3 [36000/60000 (60%)]\tLoss: 0.742786\n",
      "Train Epoch: 3 [42000/60000 (70%)]\tLoss: 0.656643\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.456996\n",
      "Train Epoch: 3 [54000/60000 (90%)]\tLoss: 0.670595\n",
      "\n",
      "Test set: Average loss: 0.6474, Accuracy: 8305/10000 (83%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.366664\n",
      "Train Epoch: 4 [6000/60000 (10%)]\tLoss: 0.308420\n",
      "Train Epoch: 4 [12000/60000 (20%)]\tLoss: 0.687408\n",
      "Train Epoch: 4 [18000/60000 (30%)]\tLoss: 0.529648\n",
      "Train Epoch: 4 [24000/60000 (40%)]\tLoss: 0.347271\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 0.606839\n",
      "Train Epoch: 4 [36000/60000 (60%)]\tLoss: 0.601362\n",
      "Train Epoch: 4 [42000/60000 (70%)]\tLoss: 0.558565\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.440675\n",
      "Train Epoch: 4 [54000/60000 (90%)]\tLoss: 0.617183\n",
      "\n",
      "Test set: Average loss: 0.5792, Accuracy: 8556/10000 (86%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.334060\n",
      "Train Epoch: 5 [6000/60000 (10%)]\tLoss: 0.288495\n",
      "Train Epoch: 5 [12000/60000 (20%)]\tLoss: 0.576381\n",
      "Train Epoch: 5 [18000/60000 (30%)]\tLoss: 0.453530\n",
      "Train Epoch: 5 [24000/60000 (40%)]\tLoss: 0.321287\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 0.587124\n",
      "Train Epoch: 5 [36000/60000 (60%)]\tLoss: 0.559249\n",
      "Train Epoch: 5 [42000/60000 (70%)]\tLoss: 0.439521\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.402065\n",
      "Train Epoch: 5 [54000/60000 (90%)]\tLoss: 0.549711\n",
      "\n",
      "Test set: Average loss: 0.5107, Accuracy: 8745/10000 (87%)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accs = {}\n",
    "epochs = 5\n",
    "for name, optimizer in optimizers.items():\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "    acc = []\n",
    "    print(f\"Training using the optimezer '{name}'\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch)\n",
    "        a = test(model, device, test_loader)\n",
    "        acc.append(a)\n",
    "        scheduler.step()\n",
    "    accs[name] = acc\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "for name in optimizers:\n",
    "    plt.plot(1 - np.array(accs[name]), label=name)\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Error rate\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASGD [0.9606, 0.9606, 0.9609, 0.961, 0.9613]\n",
      "SGD [0.9612, 0.9611, 0.9612, 0.9612, 0.9613]\n",
      "Adagrad [0.9603, 0.9606, 0.9606, 0.9609, 0.9608]\n",
      "Adam [0.6193, 0.5023, 0.5868, 0.6051, 0.7606]\n",
      "Adamax [0.778, 0.7919, 0.8305, 0.8556, 0.8745]\n"
     ]
    }
   ],
   "source": [
    "for name, vals in accs.items():\n",
    "    print(name, vals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
